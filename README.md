# cultural-qa-llm
This project explores LLM-based approaches to cultural question answering using Meta’s Llama-3-8B model. The goal is to build and evaluate systems that can correctly interpret cultural and linguistic cues across multiple cultural contexts.  We address both multiple-choice (MCQ) and short-answer (SAQ) tasks as defined in the Codabench benchmark, investigating techniques such as prompt engineering, parameter-efficient fine-tuning, self-consistency, and retrieval-augmented reasoning—while strictly adhering to the constraint of using Llama-3-8B as the sole language model.  Model performance is evaluated using benchmark accuracy, with particular attention to generalization across cultures and the limitations of exact-match evaluation for generative answers.
