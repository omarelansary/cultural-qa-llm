# Model Configuration
task: "mcq"
model_name: "meta-llama/Meta-Llama-3-8B"
quantization: "4bit"

# Data Paths
data_path: "data/train_dataset_mcq.csv"
output_dir: "output/mcq_baseline"
max_length: 512

# Training Hyperparameters
train_args:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  num_train_epochs: 3
  learning_rate: 2.0e-5
  logging_steps: 10
  save_steps: 100
  seed: 42
